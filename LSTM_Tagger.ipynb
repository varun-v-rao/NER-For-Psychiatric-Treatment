{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Tagger"
      ],
      "metadata": {
        "id": "d9p62Nu83MYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence Labeling With LSTMs.\n",
        "\n",
        "For sequence labeling, our goal is to predict labels for each token: Barack Obama went to DC -> B-PER I-PER O O B-LOC. Two considerations:\n",
        "\n",
        "1. **Input**: for sentence classification, we were using representation for the entire sentence (using conv/LSTM) as the input. For sequence labeling, we will use the LSTM hidden states for each token.\n",
        "\n",
        "2. **Loss**: The loss is the sum of individual token losses (using the cross entropy loss). Imagine doing n (sentence length) classifications for a sentence instead of a single one.\n",
        "\n",
        "We will use the ADE (adverse drug event) dataset from assignment 3 (this is already explained). We have two CoNLL format files: review_train and review_valid.  A CoNLL file is just text file where each line (except lines starting with #, which are comments) contains a token and its tag (class label). \n",
        "\n",
        "For example, here are two sentences from the train file:\n",
        "\n",
        "```\n",
        "# lexapro.Post118.Sentence2\n",
        "Lexapro\tO\n",
        "keeps\tO\n",
        "me\tO\n",
        "out\tO\n",
        "of\tO\n",
        "deep\tB-SSI\n",
        "depression\tI-SSI\n",
        ".\tO\n",
        "# zoloft.Post150.Sentence3\n",
        "Works\tO\n",
        "for\tO\n",
        "my\tO\n",
        "form\tO\n",
        "of\tO\n",
        "depression\tB-SSI\n",
        ",\tO\n",
        "however\tO\n",
        "it\tO\n",
        "has\tO\n",
        "destroyed\tB-AE\n",
        "my\tI-AE\n",
        "sleeping\tI-AE\n",
        "patterns\tI-AE\n",
        ".\tO\n",
        "# lexapro.Post175.Sentence6\n",
        "I\tO\n",
        "started\tO\n",
        "off\tO\n",
        "with\tO\n",
        "```\n",
        "\n",
        "Let's start by downloading the data."
      ],
      "metadata": {
        "id": "XJ5kPX8IMOp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"review_data\"):\n",
        "  !wget https://www.dropbox.com/s/yqgff7de73iwosr/review_data.zip?dl=1 -O review_data.zip\n",
        "  !unzip review_data.zip\n",
        "  !ls review_data"
      ],
      "metadata": {
        "id": "iOvGJkICiZqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c812941-6bf7-4d48-8320-36df773b8e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 01:55:01--  https://www.dropbox.com/s/yqgff7de73iwosr/review_data.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/yqgff7de73iwosr/review_data.zip [following]\n",
            "--2023-04-11 01:55:01--  https://www.dropbox.com/s/dl/yqgff7de73iwosr/review_data.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc936c6397c568edd46812a6d81a.dl.dropboxusercontent.com/cd/0/get/B59lcGjdfEuoCmzecJQUp9f8Ea6l9tCiif1cJBpzPEcWrRHedrm5fYHqwJnxf2xvYNrqjH1pJ1_2ZB5-WvcmMsfgCdoAlaFxPWtWzt6YZqqm46KI027dqr4Oxvp0OLc9sG1HmR_jv0hCkhEe1jqsR0xTeMmeomrtvPKUiBJH47kGNXUqpl2aNKmFR3IsRnJk-wc/file?dl=1# [following]\n",
            "--2023-04-11 01:55:02--  https://uc936c6397c568edd46812a6d81a.dl.dropboxusercontent.com/cd/0/get/B59lcGjdfEuoCmzecJQUp9f8Ea6l9tCiif1cJBpzPEcWrRHedrm5fYHqwJnxf2xvYNrqjH1pJ1_2ZB5-WvcmMsfgCdoAlaFxPWtWzt6YZqqm46KI027dqr4Oxvp0OLc9sG1HmR_jv0hCkhEe1jqsR0xTeMmeomrtvPKUiBJH47kGNXUqpl2aNKmFR3IsRnJk-wc/file?dl=1\n",
            "Resolving uc936c6397c568edd46812a6d81a.dl.dropboxusercontent.com (uc936c6397c568edd46812a6d81a.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc936c6397c568edd46812a6d81a.dl.dropboxusercontent.com (uc936c6397c568edd46812a6d81a.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 507026 (495K) [application/binary]\n",
            "Saving to: ‘review_data.zip’\n",
            "\n",
            "review_data.zip     100%[===================>] 495.14K   701KB/s    in 0.7s    \n",
            "\n",
            "2023-04-11 01:55:04 (701 KB/s) - ‘review_data.zip’ saved [507026/507026]\n",
            "\n",
            "Archive:  review_data.zip\n",
            " extracting: review_data/Icon        \n",
            "  inflating: review_data/REVIEW_LABELSEQ.txt  \n",
            "  inflating: review_data/REVIEW_LABELSTR.txt  \n",
            "  inflating: review_data/REVIEW_TEXT.txt  \n",
            "  inflating: review_data/TEST_REVIEW_TEXT.txt  \n",
            "  inflating: review_data/review_train.conll  \n",
            "  inflating: review_data/review_valid.conll  \n",
            "  inflating: review_data/review_test.conll  \n",
            "Icon\t\t     review_test.conll\t review_valid.conll\n",
            "REVIEW_LABELSEQ.txt  REVIEW_TEXT.txt\t TEST_REVIEW_TEXT.txt\n",
            "REVIEW_LABELSTR.txt  review_train.conll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will start by defining the model: an LSTM based tagger. Some definitions follow:\n",
        "1. B  = batch size\n",
        "2. T = time dimension (seq length or number of words)\n",
        "3. e = embedding dim (300 for word2vec)\n",
        "4. h = rnn_hidden_dim *2 if bidirectional else rnn_hidden_dim. \n",
        "The output of this network will be a (B x T x num_classes) tensor.\n",
        "\n",
        "What does the input look like? As before, each sentence is a sequence of token ids in the vocabulary (see the first parts of the [previous colab tutorial](https://bit.ly/lhs712w23_feb13))\n",
        "\n",
        "1. input = A batch of sentences (B x T) tensor. \n",
        "2. embedding layer (input) -> (B x T x e) tensor.\n",
        "3. LSTM(embedding output) -> (B x T x h) tensor if bidirectional = False else (B x T x 2h) tensor.\n",
        "4. Now, we want a linear layer that will take a representation for each word and output a probability for each class. To acieve this, we `reshape` the (B x T x h) tensor to a (B*T x h) tensor.  \n",
        "5. A dense layer (h x num_classes) on top of the reshaped tensor -> this will output a tensor of dimension (B*T, num_classes).\n",
        "6. As for the output, we can reshape the output from the last part into (B x T x num_classes): this will give us the probabilities we are looking for.\n",
        "\n"
      ],
      "metadata": {
        "id": "2YQKBU5dzfdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.functional import softmax\n",
        "import torch\n",
        "torch.manual_seed(42) # hitch\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, embeddings, num_classes, embed_dim, rnn_hidden_dim, rnn_layers=1, bidirectional=False, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.LSTM(embed_dim,\n",
        "                                 rnn_hidden_dim,\n",
        "                                 rnn_layers,\n",
        "                                 dropout=dropout,\n",
        "                                 bidirectional=bidirectional,\n",
        "                                 batch_first=True)\n",
        "        nn.init.orthogonal_(self.rnn.weight_hh_l0)\n",
        "        nn.init.orthogonal_(self.rnn.weight_ih_l0)\n",
        "        self.linear_in = rnn_hidden_dim if not bidirectional else rnn_hidden_dim * 2\n",
        "        self.num_classes = num_classes\n",
        "        self.dense = nn.Linear(self.linear_in, self.num_classes)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        one_hots, lengths = inputs\n",
        "        embed = self.dropout(self.embeddings(one_hots))\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.tolist(), \n",
        "                                                         batch_first=True) \n",
        "        h_is, (h_n, c_n) = self.rnn(packed) \n",
        "        h_is, _ = torch.nn.utils.rnn.pad_packed_sequence(h_is, batch_first=True) # h_is is now (B x l x h) where l = max(sentence lengths in the batch)\n",
        "        linear = self.dense(h_is)\n",
        "        # we can either apply the classifier on top of each of the word representation \n",
        "        # i.e., the B x l x h tensor. Or we can flatten the B x l x h tensor to \n",
        "        # a (B * l) x h tensor.  \n",
        "        # h_is_reshaped = h_is.reshape((h_is.shape[0] * h_is.shape[1]), -1)\n",
        "        # linear = self.dense(h_is_reshaped)\n",
        "        # linear = linear.reshape(h_is.shape[0], h_is.shape[1], self.num_classes)\n",
        "        return softmax(linear, dim=-1)\n",
        "    \n",
        "    def predict(self, inputs):\n",
        "        h_is, (h_n, c_n) = self.rnn(self.embeddings(inputs))\n",
        "        return self.dense(h_is).max(-1)[1]    "
      ],
      "metadata": {
        "id": "ss9IdOX3PrzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we train this LSTM model, we need to make sure we process the data correctly. A couple of things to notice:\n",
        "1. We read a CoNLL file instead of a normal text one.\n",
        "2. The data is **right padded**, IOW, the `<PAD>` token is added at the end of a sentnce to pad it to the max length.\n",
        "3. We have 5 labels: `<PAD>`, `B-AE`, `I-AE`, `B-SSI`, `I-SSI`, `O`. "
      ],
      "metadata": {
        "id": "3GINCQh88a_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "sO2cAiSfBoFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class DictExamples(object):\n",
        "    \"\"\"This object holds a list of dictionaries, and knows how to shuffle, sort and batch them\n",
        "    \"\"\"\n",
        "    def __init__(self, example_list, do_shuffle=True, sort_key=None):\n",
        "        \"\"\"Constructor\n",
        "\n",
        "        :param example_list:  A list of examples\n",
        "        :param do_shuffle: (``bool``) Shuffle the data? Defaults to `True`\n",
        "        :param do_sort: (``bool``) Sort the data.  Defaults to `True`\n",
        "        \"\"\"\n",
        "        self.example_list = example_list\n",
        "        if do_shuffle:\n",
        "            random.shuffle(self.example_list)\n",
        "        if sort_key is not None:\n",
        "            self.example_list = sorted(self.example_list, key=lambda x: x[sort_key])\n",
        "        self.sort_key = sort_key\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"Get a single example\n",
        "\n",
        "        :param i: (``int``) simple index\n",
        "        :return: an example\n",
        "        \"\"\"\n",
        "        return self.example_list[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of examples\n",
        "\n",
        "        :return: (``int``) length of data\n",
        "        \"\"\"\n",
        "        return len(self.example_list)\n",
        "\n",
        "    def batch(self, start, batchsz):\n",
        "        \"\"\"Get a batch of data\n",
        "        :param start: (``int``) The step index\n",
        "        :param batchsz: (``int``) The batch size\n",
        "        :return batched dictionary\n",
        "        \"\"\"\n",
        "        ex = self.example_list[start]\n",
        "        keys = ex.keys()\n",
        "        batch = {}\n",
        "\n",
        "        for k in keys:\n",
        "            batch[k] = []\n",
        "        sz = len(self.example_list)\n",
        "        idx = start * batchsz\n",
        "        for i in range(batchsz):\n",
        "            if idx >= sz:\n",
        "                break\n",
        "            ex = self.example_list[idx]\n",
        "            for k in keys:\n",
        "                batch[k].append(ex[k])\n",
        "            idx += 1\n",
        "\n",
        "        for k in keys:\n",
        "            batch[k] = np.stack(batch[k])\n",
        "        return batch\n",
        "\n",
        "\n",
        "class ExampleDataFeed(object):\n",
        "\n",
        "    \"\"\"Abstract base class that works on a list of examples\n",
        "    This doesn't use any torch abstraction, but you could replace this \n",
        "    class with a torch DataLoader if you wanted.\n",
        "    \"\"\"\n",
        "    def __init__(self, examples, batchsz, **kwargs):\n",
        "        \"\"\"Constructor from a list of examples\n",
        "\n",
        "        Use the examples requested to provide data.  Options for batching and shuffling are supported,\n",
        "        along with some optional processing function pointers\n",
        "\n",
        "        :param examples: A list of examples\n",
        "        :param batchsz: Batch size per step\n",
        "        :param kwargs: See below\n",
        "\n",
        "        :Keyword Arguments:\n",
        "            * *shuffle* -- Shuffle the data per epoch? Defaults to `False`\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        self.batchsz = batchsz\n",
        "        self.shuffle = bool(kwargs.get('shuffle', False))\n",
        "        self.steps = (len(self.examples) + self.batchsz - 1) // self.batchsz\n",
        "\n",
        "    def _batch(self, i):\n",
        "        \"\"\"\n",
        "        Get a batch of data at step `i`\n",
        "        :param i: (``int``) step index\n",
        "        :return: A batch tensor x, batch tensor y\n",
        "        \"\"\"\n",
        "        batch = self.examples.batch(i, self.batchsz)\n",
        "        return batch\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._batch(i)\n",
        "\n",
        "    def __iter__(self):\n",
        "        shuffle = np.random.permutation(np.arange(self.steps)) if self.shuffle else np.arange(self.steps)\n",
        "        for i in range(self.steps):\n",
        "            si = shuffle[i]\n",
        "            yield self._batch(si)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "\n",
        "class DictVectorizer:\n",
        "    def __init__(self, field, transform_fn=None, **kwargs):\n",
        "        self.transform_fn = lambda x: x if transform_fn is None else transform_fn\n",
        "        self.field = field\n",
        "        self.mxlen = kwargs.get('mxlen', -1)\n",
        "        self.max_seen = 0\n",
        "\n",
        "    def iterable(self, tokens):\n",
        "        for tok in tokens:\n",
        "            yield self.transform_fn(tok[self.field])\n",
        "\n",
        "    def _next_element(self, tokens, vocab):\n",
        "        for atom in self.iterable(tokens):\n",
        "            yield atom, vocab[atom]\n",
        "\n",
        "    def count(self, tokens):\n",
        "        seen = 0\n",
        "        counter = Counter()\n",
        "        for tok in self.iterable(tokens):\n",
        "            counter[tok] += 1\n",
        "            seen += 1\n",
        "        self.max_seen = max(self.max_seen, seen)\n",
        "        return counter\n",
        "\n",
        "    def run(self, tokens, vocab):\n",
        "        if self.mxlen < 0:\n",
        "            self.mxlen = self.max_seen\n",
        "        vec1d = np.zeros(self.mxlen, dtype=int)\n",
        "        tok1d = ['<PAD>'] * self.mxlen\n",
        "        i = 0\n",
        "        for i, (token, token_index) in enumerate(self._next_element(tokens, vocab)):\n",
        "            if i == self.mxlen:\n",
        "                i -= 1\n",
        "                break\n",
        "            vec1d[i] = token_index\n",
        "            tok1d[i] = token\n",
        "        valid_length = i + 1\n",
        "        return tok1d, vec1d, valid_length\n",
        "\n",
        "    def get_length(self):\n",
        "        return self.mxlen\n",
        "\n",
        "\n",
        "class CoNLLSeqReader(object):\n",
        "\n",
        "    def __init__(self, train_file, valid_file, test_file, mxlen=-1):\n",
        "        self.train_file = train_file\n",
        "        self.valid_file = valid_file\n",
        "        self.test_file = test_file\n",
        "        self.text_vectorizer = DictVectorizer(field='text', mxlen=mxlen)\n",
        "        self.label_vectorizer = DictVectorizer(field='y', mxlen=mxlen)\n",
        "        self.vocab = Counter()\n",
        "        self.label2index = {\"PAD\": 0}\n",
        "        self.build_vocab([train_file, valid_file, test_file])\n",
        "\n",
        "    def build_vocab(self, files):\n",
        "        labels = Counter()\n",
        "        for file in files:\n",
        "            if file is None:\n",
        "                continue\n",
        "            examples = self.read_examples(file)\n",
        "            for example in examples:\n",
        "                labels.update(self.label_vectorizer.count(example))\n",
        "                self.vocab.update(self.text_vectorizer.count(example))\n",
        "        for i, k in enumerate(labels.keys()):\n",
        "          self.label2index[k] = i + 1\n",
        "      \n",
        "    @staticmethod\n",
        "    def read_examples(tsfile: str):\n",
        "        tokens = []\n",
        "        examples = []\n",
        "        sentence_id = None\n",
        "        with codecs.open(tsfile, encoding='utf-8', mode='r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if line.startswith('#'):  # The following lines will have this sentence id\n",
        "                    sentence_id = line.strip().split()[1]\n",
        "                    continue\n",
        "                splits = re.split(\"\\\\s+\", line.strip())\n",
        "                if len(splits) == 1:\n",
        "                    if len(tokens) > 0:\n",
        "                        examples.append(tokens)\n",
        "                        tokens = []\n",
        "                    continue\n",
        "                assert sentence_id is not None, \"Sentence id is not set\"\n",
        "                token = {\"text\": splits[0], \"y\": splits[1], \"sentence_id\": sentence_id}\n",
        "                tokens.append(token)\n",
        "            if len(tokens) > 0:\n",
        "                examples.append(tokens)\n",
        "        return examples\n",
        "\n",
        "    def load(self, filename, batchsz, shuffle=False):\n",
        "        ts = []\n",
        "        texts = self.read_examples(filename)\n",
        "        sort_key = \"text_lengths\"\n",
        "        for i, example_tokens in enumerate(texts):\n",
        "            example = {}\n",
        "            example[\"token\"], example[\"text\"], example[\"text_lengths\"] = self.text_vectorizer.run(example_tokens, self.vocab)\n",
        "            example[\"label\"], example['y'], example[\"y_lengths\"] = self.label_vectorizer.run(example_tokens, self.label2index)\n",
        "            example['ids'] = example_tokens[0]['sentence_id']\n",
        "            ts.append(example)\n",
        "        examples = DictExamples(ts, do_shuffle=shuffle, sort_key=sort_key)\n",
        "        return ExampleDataFeed(examples, batchsz=batchsz, shuffle=shuffle)"
      ],
      "metadata": {
        "id": "s82OONv8LpCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Layer"
      ],
      "metadata": {
        "id": "jmehcw6CBrZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk5ImOYjjxHC"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def init_embeddings(vocab_size, embed_dim, unif):\n",
        "    return np.random.uniform(-unif, unif, (vocab_size, embed_dim))\n",
        "    \n",
        "\n",
        "class EmbeddingsReader:\n",
        "    @staticmethod\n",
        "    def load(filename, vocab, unif=0.25):\n",
        "        def read_word(f):\n",
        "            s = bytearray()\n",
        "            ch = f.read(1)\n",
        "            while ch != b' ':\n",
        "                s.extend(ch)\n",
        "                ch = f.read(1)\n",
        "            s = s.decode('utf-8')\n",
        "            # Only strip out normal space and \\n not other spaces which are words.\n",
        "            return s.strip(' \\n')\n",
        "\n",
        "        vocab_size = len(vocab)\n",
        "        with io.open(filename, \"rb\") as f:\n",
        "            header = f.readline()\n",
        "            file_vocab_size, embed_dim = map(int, header.split())\n",
        "            weight = init_embeddings(len(vocab), embed_dim, unif)\n",
        "            if '[PAD]' in vocab:\n",
        "                weight[vocab['[PAD]']] = 0.0\n",
        "            width = 4 * embed_dim\n",
        "            for i in range(file_vocab_size):\n",
        "                word = read_word(f)\n",
        "                raw = f.read(width)\n",
        "                if word in vocab:\n",
        "                    vec = np.frombuffer(raw, dtype=np.float32)\n",
        "                    weight[vocab[word]] = vec\n",
        "        embeddings = nn.Embedding(weight.shape[0], weight.shape[1])\n",
        "        embeddings.weight = nn.Parameter(torch.from_numpy(weight).float())\n",
        "        return embeddings, embed_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "fu1noQ1GBvSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF88Hw7oLsO_"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "class ConfusionMatrix:\n",
        "    \"\"\"Confusion matrix with metrics\n",
        "\n",
        "    This class accumulates classification output, and tracks it in a confusion matrix.\n",
        "    Metrics are available that use the confusion matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, labels):\n",
        "        \"\"\"Constructor with input labels\n",
        "\n",
        "        :param labels: Either a dictionary (`k=int,v=str`) or an array of labels\n",
        "        \"\"\"\n",
        "        if type(labels) is dict:\n",
        "            self.labels = []\n",
        "            for i in range(len(labels)):\n",
        "                self.labels.append(labels[i])\n",
        "        else:\n",
        "            self.labels = labels\n",
        "        self.labels = [str(x) for x in labels]\n",
        "        nc = len(self.labels)\n",
        "        self._cm = np.zeros((nc, nc), dtype=int)\n",
        "\n",
        "    def add(self, truth, guess):\n",
        "        \"\"\"Add a single value to the confusion matrix based off `truth` and `guess`\n",
        "\n",
        "        :param truth: The real `y` value (or ground truth label)\n",
        "        :param guess: The guess for `y` value (or assertion)\n",
        "        \"\"\"\n",
        "\n",
        "        self._cm[truth, guess] += 1\n",
        "\n",
        "    def __str__(self):\n",
        "        values = []\n",
        "        width = max(8, max(len(x) for x in self.labels) + 1)\n",
        "        for i, label in enumerate([''] + self.labels):\n",
        "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "        values += ['\\n']\n",
        "        for i, label in enumerate(self.labels):\n",
        "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
        "            for j in range(len(self.labels)):\n",
        "                values += [\"{:{width}d}\".format(self._cm[i, j], width=width + 1)]\n",
        "            values += ['\\n']\n",
        "        values += ['\\n']\n",
        "        return ''.join(values)\n",
        "\n",
        "    def save(self, outfile):\n",
        "        ordered_fieldnames = OrderedDict([(\"labels\", None)] + [(l, None) for l in self.labels])\n",
        "        with open(outfile, 'w') as f:\n",
        "            dw = csv.DictWriter(f, delimiter=',', fieldnames=ordered_fieldnames)\n",
        "            dw.writeheader()\n",
        "            for index, row in enumerate(self._cm):\n",
        "                row_dict = {l: row[i] for i, l in enumerate(self.labels)}\n",
        "                row_dict.update({\"labels\": self.labels[index]})\n",
        "                dw.writerow(row_dict)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the matrix\n",
        "        \"\"\"\n",
        "        self._cm *= 0\n",
        "\n",
        "    def get_correct(self):\n",
        "        \"\"\"Get the diagonals of the confusion matrix\n",
        "\n",
        "        :return: (``int``) Number of correct classifications\n",
        "        \"\"\"\n",
        "        return self._cm.diagonal().sum()\n",
        "\n",
        "    def get_total(self):\n",
        "        \"\"\"Get total classifications\n",
        "\n",
        "        :return: (``int``) total classifications\n",
        "        \"\"\"\n",
        "        return self._cm.sum()\n",
        "\n",
        "    def get_acc(self):\n",
        "        \"\"\"Get the accuracy\n",
        "\n",
        "        :return: (``float``) accuracy\n",
        "        \"\"\"\n",
        "        return float(self.get_correct())/self.get_total()\n",
        "\n",
        "    def get_recall(self):\n",
        "        \"\"\"Get the recall\n",
        "\n",
        "        :return: (``float``) recall\n",
        "        \"\"\"\n",
        "        total = np.sum(self._cm, axis=1)\n",
        "        total = (total == 0) + total\n",
        "        return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "    def get_support(self):\n",
        "        return np.sum(self._cm, axis=1)\n",
        "\n",
        "    def get_precision(self):\n",
        "        \"\"\"Get the precision\n",
        "        :return: (``float``) precision\n",
        "        \"\"\"\n",
        "\n",
        "        total = np.sum(self._cm, axis=0)\n",
        "        total = (total == 0) + total\n",
        "        return np.diag(self._cm) / total.astype(float)\n",
        "\n",
        "    def get_mean_precision(self):\n",
        "        \"\"\"Get the mean precision across labels\n",
        "\n",
        "        :return: (``float``) mean precision\n",
        "        \"\"\"\n",
        "        return np.mean(self.get_precision())\n",
        "\n",
        "    def get_weighted_precision(self):\n",
        "        return np.sum(self.get_precision() * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_mean_recall(self):\n",
        "        \"\"\"Get the mean recall across labels\n",
        "\n",
        "        :return: (``float``) mean recall\n",
        "        \"\"\"\n",
        "        return np.mean(self.get_recall())\n",
        "\n",
        "    def get_weighted_recall(self):\n",
        "        return np.sum(self.get_recall() * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_weighted_f(self, beta=1):\n",
        "        return np.sum(self.get_class_f(beta) * self.get_support())/float(self.get_total())\n",
        "\n",
        "    def get_macro_f(self, beta=1):\n",
        "        \"\"\"Get the macro F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "        :param beta: (``float``) defaults to 1 (F1)\n",
        "        :return: (``float``) macro F_b\n",
        "        \"\"\"\n",
        "        if beta < 0:\n",
        "            raise Exception('Beta must be greater than 0')\n",
        "        return np.mean(self.get_class_f(beta))\n",
        "\n",
        "    def get_class_f(self, beta=1):\n",
        "        p = self.get_precision()\n",
        "        r = self.get_recall()\n",
        "\n",
        "        b = beta*beta\n",
        "        d = (b * p + r)\n",
        "        d = (d == 0) + d\n",
        "\n",
        "        return (b + 1) * p * r / d\n",
        "\n",
        "    def get_f(self, beta=1):\n",
        "        \"\"\"Get 2 class F_b, with adjustable beta (defaulting to F1)\n",
        "\n",
        "        :param beta: (``float``) defaults to 1 (F1)\n",
        "        :return: (``float``) 2-class F_b\n",
        "        \"\"\"\n",
        "        p = self.get_precision()[1]\n",
        "        r = self.get_recall()[1]\n",
        "        if beta < 0:\n",
        "            raise Exception('Beta must be greater than 0')\n",
        "        d = (beta*beta * p + r)\n",
        "        if d == 0:\n",
        "            return 0\n",
        "        return (beta*beta + 1) * p * r / d\n",
        "\n",
        "    def get_all_metrics(self):\n",
        "        \"\"\"Make a map of metrics suitable for reporting, keyed by metric name\n",
        "\n",
        "        :return: (``dict``) Map of metrics keyed by metric names\n",
        "        \"\"\"\n",
        "        metrics = {'acc': self.get_acc()}\n",
        "        # If 2 class, assume second class is positive AKA 1\n",
        "        if len(self.labels) == 2:\n",
        "            metrics['precision'] = self.get_precision()[1]\n",
        "            metrics['recall'] = self.get_recall()[1]\n",
        "            metrics['f1'] = self.get_f(1)\n",
        "        else:\n",
        "            metrics['mean_precision'] = self.get_mean_precision()\n",
        "            metrics['mean_recall'] = self.get_mean_recall()\n",
        "            metrics['macro_f1'] = self.get_macro_f(1)\n",
        "            metrics['weighted_precision'] = self.get_weighted_precision()\n",
        "            metrics['weighted_recall'] = self.get_weighted_recall()\n",
        "            metrics['weighted_f1'] = self.get_weighted_f(1)\n",
        "        return metrics\n",
        "\n",
        "    def add_batch(self, truth, guess):\n",
        "        \"\"\"Add a batch of data to the confusion matrix\n",
        "\n",
        "        :param truth: The truth tensor\n",
        "        :param guess: The guess tensor\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for truth_i, guess_i in zip(truth, guess):\n",
        "            self.add(truth_i, guess_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how this works, look at the following code:"
      ],
      "metadata": {
        "id": "1TyyiSx2-nIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Data"
      ],
      "metadata": {
        "id": "E9kPb0_BB2-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    train_file, valid_file, test_file = [\"review_data/review_train.conll\", \"review_data/review_valid.conll\",\n",
        "                                         \"review_data/review_test.conll\"]\n",
        "    MAXLEN = -1\n",
        "    ade_reader = CoNLLSeqReader(train_file, valid_file, test_file, mxlen=MAXLEN)\n",
        "    batchsz = 16\n",
        "    ade_train_dl = ade_reader.load(filename=train_file, batchsz=batchsz, shuffle=False)\n",
        "    ade_dev_dl = ade_reader.load(filename=valid_file, batchsz=batchsz, shuffle=False)\n",
        "    ade_test_dl = ade_reader.load(filename=valid_file, batchsz=batchsz, shuffle=False)\n",
        "    print(ade_reader.label2index)\n",
        "    print(f\"{len(ade_reader.vocab)} tokens in vocab\")\n",
        "    # for item in ade_test_dl:\n",
        "    #   if len(set(item['y_lengths'])) > 1:\n",
        "    #     print(item)\n",
        "    #     break"
      ],
      "metadata": {
        "id": "nyFY0p8v-sXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4a2793-7b82-47cd-f1a7-27219850a62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PAD': 0, 'B-AE': 1, 'I-AE': 2, 'O': 3, 'B-SSI': 4, 'I-SSI': 5}\n",
            "9837 tokens in vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will have to change the trainer and evaluator a little bit as well."
      ],
      "metadata": {
        "id": "RZYWZdq0ACjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "rotP2tTTB9vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, optimizer: torch.optim.Optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def run(self, model, labels, data_loader, loss_fn): \n",
        "        model.train()\n",
        "        cm = ConfusionMatrix(labels)\n",
        "        for batch in tqdm(data_loader):\n",
        "            loss_value, y_pred, y_actual = self.update(model, loss_fn, batch)\n",
        "            _, best = y_pred.max(1)\n",
        "            yt = y_actual.cpu().int().numpy()\n",
        "            yp = best.cpu().int().numpy()\n",
        "            cm.add_batch(yt, yp)\n",
        "        # print(cm)\n",
        "        print(cm.get_all_metrics())\n",
        "        print(\"training loss:\", loss_value)\n",
        "        print(\"-\"*30)\n",
        "        return cm\n",
        "    \n",
        "    def update(self, model, loss_fn, batch):\n",
        "        self.optimizer.zero_grad()\n",
        "        x, lengths, y = torch.LongTensor(batch[\"text\"]), torch.LongTensor(batch[\"text_lengths\"]), torch.LongTensor(batch[\"y\"])\n",
        "        \n",
        "        lengths_sorted, perm_idx = lengths.sort(dim=0, descending=True)\n",
        "        x_sorted = x[perm_idx]\n",
        "        inputs = (x_sorted.to('cpu'), lengths_sorted)\n",
        "        \n",
        "        y_true_sorted = y[perm_idx]\n",
        "        y_true_sorted = y_true_sorted[:, :max(lengths_sorted)]\n",
        "        y_pred = model(inputs) # this is B x l x num_classes where l is the max len of the batch\n",
        "        assert y_true_sorted.shape[0] == y_pred.shape[0] and y_true_sorted.shape[1] == y_pred.shape[1]\n",
        "        y_true_sorted = y_true_sorted.reshape(y_true_sorted.shape[0]*y_true_sorted.shape[1]).to('cpu')\n",
        "        y_pred = y_pred.reshape(y_pred.shape[0]*y_pred.shape[1], -1).to('cpu')\n",
        "        loss_value = loss_fn(y_pred, y_true_sorted)\n",
        "        loss_value.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss_value.item(), y_pred, y_true_sorted\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def run(self, model, labels, data_loader):\n",
        "        model.eval()\n",
        "        cm = ConfusionMatrix(labels)\n",
        "        for batch in tqdm(data_loader):\n",
        "            y_pred, y_true, _, _, _ = self.inference(model, batch)\n",
        "            _, yp = y_pred.max(-1)\n",
        "            yp = yp.flatten().cpu().int().numpy()\n",
        "            yt = y_true.flatten().cpu().int().numpy()\n",
        "            cm.add_batch(yt, yp)\n",
        "        return cm\n",
        "\n",
        "    def inference(self, model, batch):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x, lengths, y, token, ids = torch.LongTensor(batch[\"text\"]), \\\n",
        "            torch.LongTensor(batch[\"text_lengths\"]), \\\n",
        "            torch.LongTensor(batch[\"y\"]), \\\n",
        "            batch[\"token\"], \\\n",
        "            batch[\"ids\"]\n",
        "            lengths_sorted, perm_idx = lengths.sort(0, descending=True)\n",
        "            x_sorted = x[perm_idx]\n",
        "            token_sorted = token[perm_idx]\n",
        "            ids_sorted = ids[perm_idx]\n",
        "            y_true_sorted = y[perm_idx]        \n",
        "            y_true_sorted = y_true_sorted[:, :max(lengths_sorted)]\n",
        "            inputs = (x_sorted.to('cpu'), lengths_sorted)\n",
        "            y_pred = model(inputs)    \n",
        "            return y_pred, y_true_sorted, lengths_sorted, token_sorted, ids_sorted\n"
      ],
      "metadata": {
        "id": "kjDCp059AQjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tie everything together in a fit fn."
      ],
      "metadata": {
        "id": "417hqqsEFxtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def fit(model, labels, optimizer, loss_fn, epochs, train_data_loader, dev_data_loader, test_data_loader):\n",
        "    trainer = Trainer(optimizer)\n",
        "    evaluator = Evaluator()\n",
        "    best_macro_f = 0.0\n",
        "    test = True\n",
        "    for epoch in range(epochs):\n",
        "        print('EPOCH {}'.format(epoch + 1))\n",
        "        print('=================================')\n",
        "        print('Training Results')\n",
        "        cm = trainer.run(model, labels, train_data_loader, loss_fn)\n",
        "        print('Validation Results')\n",
        "        cm = evaluator.run(model, labels, dev_data_loader)\n",
        "        # print(cm)\n",
        "        print(cm.get_all_metrics())\n",
        "        if cm.get_macro_f() > best_macro_f:\n",
        "            print('New best model {:.2f}'.format(cm.get_macro_f()))\n",
        "            best_macro_f = cm.get_macro_f()\n",
        "            torch.save(model.state_dict(), './checkpoint.pyt')\n",
        "    if test:\n",
        "        model.load_state_dict(torch.load('./checkpoint.pyt'))\n",
        "        cm = evaluator.run(model, labels, test_data_loader)\n",
        "        print('Final result')\n",
        "        print(cm)\n",
        "        print(cm.get_all_metrics())\n",
        "    return cm.get_macro_f()"
      ],
      "metadata": {
        "id": "1eBYch54F3z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "embed_dim = 300\n",
        "emb_random = nn.Embedding(len(ade_reader.vocab), embed_dim)\n",
        "import os\n",
        "do_emb_word2vec = False\n",
        "if do_emb_word2vec:\n",
        "  if not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n",
        "    # download the word2vec file and unzip\n",
        "    !wget https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
        "    !mv 'GoogleNews-vectors-negative300.bin.gz?dl=1' GoogleNews-vectors-negative300.bin.gz\n",
        "    print(\"file downloaded, unzipping...\")\n",
        "    !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "    print(\"unzipped\")\n",
        "\n",
        "\n",
        "  PRETRAINED_EMBEDDINGS_FILE = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "  # generate the embeddings for our dataset. The word2vec file has embeddings for \n",
        "  # 30K tokens, but our embedding matrix will only hold the weights for the tokens\n",
        "  # that are in our vocab (i.e., all unique tokens in train + dev + test data).\n",
        "\n",
        "  emb_word2vec, embed_dim = EmbeddingsReader.load(PRETRAINED_EMBEDDINGS_FILE, \n",
        "                                                        ade_reader.vocab)"
      ],
      "metadata": {
        "id": "8-GlZMOVwmvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_hidden_dim = 200\n",
        "rnn_layers = 1\n",
        "bidirectional = True\n",
        "model  = LSTMTagger(embeddings=emb_random, \n",
        "                    num_classes=len(ade_reader.label2index.keys()), \n",
        "                    embed_dim=embed_dim, rnn_hidden_dim=rnn_hidden_dim, \n",
        "                    bidirectional=bidirectional,\n",
        "                    rnn_layers=rnn_layers)\n",
        "model.to('cpu')\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to('cpu')\n",
        "learnable_params = [name for name, p in model.named_parameters() if p.requires_grad]\n",
        "print(learnable_params)\n",
        "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(learnable_params, lr=10.0)\n",
        "fit(model=model, labels=list(ade_reader.label2index.values()), optimizer=optimizer, loss_fn=loss_fn, epochs=50, \n",
        "    train_data_loader=ade_train_dl, dev_data_loader=ade_dev_dl, test_data_loader=ade_test_dl)"
      ],
      "metadata": {
        "id": "yahY1d98syxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Output File"
      ],
      "metadata": {
        "id": "FWpi_AfdK49b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from baseline.utils import to_chunks\n",
        "except ImportError:\n",
        "    !pip install mead-baseline\n",
        "    from baseline.utils import to_chunks\n",
        "\n",
        "from tqdm import tqdm\n",
        "batchsz = 16\n",
        "\n",
        "def generate_labelseq(sentence_ids, all_sentence_tokens, all_sentence_labels, \n",
        "                   output_base):\n",
        "  assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_labels)\n",
        "  with open(f\"{output_base}.labelseq\", \"w\") as wf:\n",
        "    wf.write(\"ID\\tTAGSEQ\\n\")\n",
        "    for sentence_id, sentence_tokens, sentence_labels in zip(sentence_ids, all_sentence_tokens, all_sentence_labels):\n",
        "      assert len(sentence_tokens) == len(sentence_labels)\n",
        "      wf.write(f'{sentence_id}\\t{\" \".join(sentence_labels)}\\n')\n",
        "  print(f\"generated labelseq file {output_base}.labelseq\")\n",
        "  \n",
        "\n",
        "\n",
        "def predict_tags_for_file(_file, model_state_dict, reader, evaluator, output_base, output_formats=[\"human_readable\", \"labelseq\"]):\n",
        "  file_data_loader = reader.load(filename=_file, batchsz=batchsz, shuffle=False)\n",
        "  index2label = {v:k for k,v in reader.label2index.items()}\n",
        "  model = LSTMTagger(embeddings=emb_random, \n",
        "                    num_classes=len(ade_reader.label2index.keys()), \n",
        "                    embed_dim=embed_dim, rnn_hidden_dim=rnn_hidden_dim, \n",
        "                    bidirectional=bidirectional,\n",
        "                    rnn_layers=rnn_layers)\n",
        "  model.load_state_dict(torch.load(model_state_dict))\n",
        "  model.cpu()\n",
        "  model.eval()\n",
        "  all_sentence_ids = []\n",
        "  all_sentence_tokens = []\n",
        "  all_sentence_pred_labels = []\n",
        "  all_sentence_true_labels = []\n",
        "\n",
        "  for batch in tqdm(file_data_loader):\n",
        "    y_true_batch, sentence_lengths_batch, sentence_ids_batch, sentence_tokens_batch = batch[\"y\"], batch[\"text_lengths\"], batch[\"ids\"], batch[\"token\"]\n",
        "    y_pred_batch = model.predict(torch.LongTensor(batch[\"text\"]).cpu())\n",
        "    for sentence_tokens, sentence_pred_labels, sentence_true_labels, sentence_length, sentence_id in \\\n",
        "        zip(sentence_tokens_batch, y_pred_batch, y_true_batch, sentence_lengths_batch, sentence_ids_batch):\n",
        "      all_sentence_ids.append(sentence_id)\n",
        "      sentence_tokens = sentence_tokens[:sentence_length]\n",
        "      sentence_pred_labels = [index2label[k.item()] for k in sentence_pred_labels[:sentence_length]] \n",
        "      sentence_true_labels = [index2label[k.item()] for k in sentence_true_labels[:sentence_length]] \n",
        "      all_sentence_tokens.append(sentence_tokens)\n",
        "      all_sentence_true_labels.append(sentence_true_labels)\n",
        "      all_sentence_pred_labels.append(sentence_pred_labels)\n",
        "\n",
        "  if \"labelseq\" in output_formats:\n",
        "    generate_labelseq(\n",
        "        all_sentence_ids, \n",
        "        all_sentence_tokens, \n",
        "        all_sentence_pred_labels, \n",
        "        output_base\n",
        "    )\n",
        "\n",
        "# finally, run this method on the test data and look at the generated labelseq file.\n",
        "test_file=\"review_data/review_test.conll\"\n",
        "predict_tags_for_file(test_file, model_state_dict=\"./checkpoint.pyt\", \n",
        "                      reader=ade_reader, evaluator=Evaluator(), \n",
        "                      output_base=\"test_output\", output_formats=[\"labelseq\"])\n"
      ],
      "metadata": {
        "id": "3mYaxX2LtaIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ca7e47-6fa7-421a-dff4-1aeefc0d0657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:08<00:00,  9.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated labelseq file test_output.labelseq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}